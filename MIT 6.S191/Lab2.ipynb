{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yala/introdeeplearning  # Clone the full repo\n",
        "%cd introdeeplearning  # Move into the repo directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXzMGi4gb1v",
        "outputId": "ba84e22a-7e2f-4ac5-b52f-1fd177642809"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'introdeeplearning' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'introdeeplearning # Move into the repo directory'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow matplotlib pandas  # Install required packages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP_xTvCigeJy",
        "outputId": "525829bb-bd20-4f85-8175-1f1d87aaf3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload lab1_utils.py manually"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "kmUlHCjVgg0t",
        "outputId": "5be02d90-092e-4448-ba36-3e3eba27382a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36428058-bf31-48ac-912e-b13885f6539a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-36428058-bf31-48ac-912e-b13885f6539a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lab1_utils.py to lab1_utils.py\n",
            "Saving preprocessing.py to preprocessing.py\n",
            "Saving special_tokens.py to special_tokens.py\n",
            "Saving utils.py to utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzIT0MPLTpLs"
      },
      "source": [
        "# Lab Part II: RNN Sentiment Classifier\n",
        "\n",
        "In the previous lab, you built a tweet sentiment classifier with a simple feedforward neural network. Now we ask you to improve this model by representing it as a *sequence* of words, with a recurrent neural network.\n",
        "\n",
        "First import some things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QhOG1lguTpLv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pickle as p\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import utils\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cydvju-wTpLx"
      },
      "source": [
        "Our model will be like this:\n",
        "\n",
        "![alt-text](lab2-diagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLP2WRMNTpLy"
      },
      "source": [
        "We feed words one by one into LSTM layers.  After feeding in all the words, we take the final state of the LSTM and run it thorugh one fully connected layer to multiply it by a final set of weights. We specificy that this fully connected layer should have a single output, which, one sigmoid-ed, is the probability that the tweet is positive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCqwXYnfTpLz"
      },
      "source": [
        "## Step 1: Set up our Model Parameters\n",
        "\n",
        "Similarly to the last lab, we'll be training using batches. Our hidden layer will have 100 units, and we have 7597 words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()  # Disable TF2 eager execution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2pxbuBSgpbD",
        "outputId": "bd2e0308-bbaf-442e-a31b-b97b1c78757c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ttHYTGThTpL0"
      },
      "outputs": [],
      "source": [
        "# set variables\n",
        "tweet_size = 20\n",
        "hidden_size = 100\n",
        "vocab_size = 7597\n",
        "batch_size = 64\n",
        "\n",
        "# this just makes sure that all our following operations will be placed in the right graph.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# create a session variable that we can run later.\n",
        "session = tf.Session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_nFvyzgTpL1"
      },
      "source": [
        "## Step 2: Create Placeholders\n",
        "\n",
        "We need to create placeholders for variable data that we will feed in ourselves (aka our tweets). Placeholders allow us to incorporate this data into the graph even though we don't know what it is yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxz3uepfTpL2"
      },
      "outputs": [],
      "source": [
        "# the placeholder for tweets has first dimension batch_size for each tweet in a batch,\n",
        "# second dimension tweet_size for each word in the tweet, and third dimension vocab_size\n",
        "# since each word itself is represented by a one-hot vector of size vocab_size.\n",
        "# Note that we use 'None' instead of batch_size for the first dimsension.  This allows us\n",
        "# to deal with variable batch sizes\n",
        "tweets = tf.placeholder(tf.float32, [None, tweet_size, vocab_size])\n",
        "\n",
        "'''TODO: create a placeholder for the labels (our predictions).\n",
        "   This should be a 1D vector with size = None,\n",
        "   since we are predicting one value for each tweet in the batch,\n",
        "   but we want to be able to deal with variable batch sizes.''';\n",
        "\n",
        "labels = tf.placeholder(tf.float32, [None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtsQafohTpL3"
      },
      "source": [
        "## Step 3: Build the LSTM Layers\n",
        "\n",
        "We want to feed the input sequence, word by word, into an LSTM layer, or multiple LSTM layers (we could also call this an LSTM **encoder**).  At each \"timestep\", we feed in the next word, and the LSTM updates its cell state. The final LSTM cell state can then be fed through a final classification layer(s) to get our sentiment prediction.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg6ForQjTpL4"
      },
      "source": [
        "Now let's make our LSTM layer. The steps for this are:\n",
        "1. Create a LSTM Cell using [tf.contrib.rnn.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)\n",
        "\n",
        "2. Wrap a couple of these cells in [tf.nn.rnn_cell.MultiRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell) to create a multiple LSTM layers.\n",
        "\n",
        "2. Define the operation to run these layers with [dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define parameters\n",
        "hidden_size = 128  # Example size, adjust as needed\n",
        "batch_size = None  # For variable batch sizes\n",
        "\n",
        "'''TODO: create an LSTM Cell using BasicLSTMCell'''\n",
        "# Modern LSTM implementation (TF2/Keras)\n",
        "lstm_cell = tf.keras.layers.LSTM(\n",
        "    units=hidden_size,\n",
        "    return_sequences=False,\n",
        "    return_state=True\n",
        ")\n",
        "\n",
        "'''TODO: create three LSTM layers by wrapping in MultiRNNCell'''\n",
        "# For stacked LSTMs:\n",
        "multi_lstm_cells = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(hidden_size, return_sequences=True) for _ in range(2)\n",
        "] + [tf.keras.layers.LSTM(hidden_size, return_sequences=False, return_state=True)]\n",
        ")\n",
        "\n",
        "# Process inputs\n",
        "outputs, *states = multi_lstm_cells(tweets)\n",
        "final_state = states[-1]  # Get final state from last layer"
      ],
      "metadata": {
        "id": "e88A33yRvTmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Fr_jpJTpL6"
      },
      "source": [
        "## Step 4: Classification Layer\n",
        "\n",
        "Now we have the final state of the LSTM layers after feeding in the tweet word by word. We can take this final state and feed it into a simple classfication layer that takes the cell state, multiplies it by some weight matrix (with bias) and outputs a single value corresponding to whether it thinks the tweet is overall positive or not.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sv9ucWQQTpL6"
      },
      "outputs": [],
      "source": [
        "## We define this function that creates a weight matrix + bias parameter and uses them to do a matrix multiplication.\n",
        "def linear(input_, output_size, name, init_bias=0.0):\n",
        "    shape = input_.get_shape().as_list()\n",
        "    with tf.variable_scope(name):\n",
        "        W = tf.get_variable(\"weights\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n",
        "    if init_bias is None:\n",
        "        return tf.matmul(input_, W)\n",
        "    with tf.variable_scope(name):\n",
        "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
        "    return tf.matmul(input_, W) + b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "'''TODO: pass the final state into this linear function to multiply it\n",
        "   by the weights and add bias to get our output.\n",
        "'''\n",
        "\n",
        "# 1. Define the LSTM model using Keras API\n",
        "lstm_layer = tf.keras.layers.LSTM(\n",
        "    units=hidden_size,\n",
        "    return_sequences=False,\n",
        "    return_state=True\n",
        ")\n",
        "\n",
        "# 2. Process inputs\n",
        "outputs, *states = lstm_layer(tweets)\n",
        "final_hidden = states[-1]  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "# 3. Classification layer (no need for manual variable scoping)\n",
        "sentiment = tf.keras.layers.Dense(\n",
        "    units=1,  # Single output for binary classification\n",
        "    activation=None,  # Linear activation for logits\n",
        "    name='output_layer'\n",
        ")(final_hidden)"
      ],
      "metadata": {
        "id": "tQIK6I9qzQNC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cP4XV3tTpL7"
      },
      "source": [
        "## Step 5: Define Loss\n",
        "\n",
        "Now we define a loss function that we'll use to determine the difference between what we predicted and what's actually correct.  We'll want to use cross entropy, since we can take into account what probability the model gave to the a tweet being positive.\n",
        "\n",
        "The output we just got from the linear classification layer is called a 'logit' -- the raw value before transforming it into a probability between 0 and 1.  We can feed these logits to  [`tf.nn.sigmoid_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits), which will take the sigmoid of these logits (making them between 0 and 1) and then calculate the cross-entropy with the ground truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": true,
        "id": "-aquPFefTpL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f19ea5-152b-4448-ef29-57577c14a954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py:1260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Safely handle tensor shapes\n",
        "if sentiment.shape.ndims == 2 and sentiment.shape[1].value == 1:\n",
        "    sentiment = tf.squeeze(sentiment, axis=[1])\n",
        "else:\n",
        "    sentiment = tf.reshape(sentiment, [-1])\n",
        "\n",
        "'''TODO: define our loss function.\n",
        "   We will use tf.nn.sigmoid_cross_entropy_with_logits, which will compare our\n",
        "   sigmoid-ed prediction (sentiment from above) to the ground truth (labels).''';\n",
        "\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
        "\n",
        "# our loss with sigmoid_cross_entropy_with_logits gives us a loss for each\n",
        "# example in the batch.  We take the mean of all these losses.\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "# to get actual results like 'positive' or 'negative' ,\n",
        "# we round the prediction probability to 0 or 1.\n",
        "prediction = tf.to_float(tf.greater_equal(sentiment, 0.5))\n",
        "\n",
        "# calculate the error based on which predictions were actually correct.\n",
        "pred_err = tf.to_float(tf.not_equal(prediction, labels))\n",
        "pred_err = tf.reduce_sum(pred_err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAdEMtESTpL8"
      },
      "source": [
        "## Step 6: Train\n",
        "\n",
        "Now we define the operation that actually changes the weights by minimizing the loss.  \n",
        "\n",
        "[`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) is just a gradient descent algorithm that uses a variable learning rate to converge faster and more effectively.\n",
        "\n",
        "We want to specify this optimizer and then call the [minimize](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#minimize) function, the optimizer knows it wants to minimize the loss we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": true,
        "id": "IvJ3uJqaTpL8"
      },
      "outputs": [],
      "source": [
        "'''Define the operation that specifies the AdamOptimizer and tells\n",
        "   it to minimize the loss.''';\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFg8wDPrTpL8"
      },
      "source": [
        "## Step 7: Run Session!\n",
        "\n",
        "Now that we've made all the variable and operations in our graph, we can load the data, feed it in, and run the model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "9gyPf6AhHXvl",
        "outputId": "365d9c23-68c9-4bf7-ef4e-1d27a5c16015"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e14fd2f7-2281-4a0a-925d-bb0c42d4d0f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e14fd2f7-2281-4a0a-925d-bb0c42d4d0f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.p to test.p\n",
            "Saving testTweets_preprocessed.json to testTweets_preprocessed.json\n",
            "Saving testTweets_preprocessed.p to testTweets_preprocessed.p\n",
            "Saving train.p to train.p\n",
            "Saving trainSentences.p to trainSentences.p\n",
            "Saving trainTweets_preprocessed.json to trainTweets_preprocessed.json\n",
            "Saving trainTweets_preprocessed.p to trainTweets_preprocessed.p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./data  # Create data directory if it doesn't exist\n",
        "!mv test.p ./data/  # Move the file\n",
        "!mv testTweets_preprocessed.json ./data/\n",
        "!mv testTweets_preprocessed.p ./data/\n",
        "!mv train.p ./data/\n",
        "!mv trainSentences.p ./data/\n",
        "!mv trainTweets_preprocessed.json ./data/\n",
        "!mv trainTweets_preprocessed.p ./data/"
      ],
      "metadata": {
        "id": "J2Z1pIOmICsy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2MzF0Y_IGrs",
        "outputId": "ba161063-1b55-498b-8fa0-8b401ada9aca"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.p\t\t\t      trainSentences.p\n",
            "testTweets_preprocessed.json  trainTweets_preprocessed.json\n",
            "testTweets_preprocessed.p     trainTweets_preprocessed.p\n",
            "train.p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "collapsed": true,
        "id": "TsENoWfaTpL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8185c07-5c04-412e-88d9-ea6a060e4e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minibatch train loss at step 0 : 0.6933515\n",
            "Minibatch train error: 57.000%\n",
            "Test loss: 0.693\n",
            "Test error: 64.857%\n",
            "Minibatch train loss at step 50 : 0.58235586\n",
            "Minibatch train error: 39.000%\n",
            "Test loss: 0.596\n",
            "Test error: 42.714%\n",
            "Minibatch train loss at step 100 : 0.5275243\n",
            "Minibatch train error: 43.000%\n",
            "Test loss: 0.554\n",
            "Test error: 40.571%\n",
            "Minibatch train loss at step 150 : 0.603336\n",
            "Minibatch train error: 45.000%\n",
            "Test loss: 0.551\n",
            "Test error: 44.571%\n",
            "Minibatch train loss at step 200 : 0.56637657\n",
            "Minibatch train error: 37.000%\n",
            "Test loss: 0.538\n",
            "Test error: 33.571%\n",
            "Minibatch train loss at step 250 : 0.5711794\n",
            "Minibatch train error: 37.000%\n",
            "Test loss: 0.535\n",
            "Test error: 32.143%\n",
            "Minibatch train loss at step 300 : 0.42023373\n",
            "Minibatch train error: 29.000%\n",
            "Test loss: 0.531\n",
            "Test error: 38.857%\n",
            "Minibatch train loss at step 350 : 0.47066242\n",
            "Minibatch train error: 27.000%\n",
            "Test loss: 0.517\n",
            "Test error: 31.571%\n",
            "Minibatch train loss at step 400 : 0.5338768\n",
            "Minibatch train error: 39.000%\n",
            "Test loss: 0.516\n",
            "Test error: 35.429%\n",
            "Minibatch train loss at step 450 : 0.52625096\n",
            "Minibatch train error: 24.000%\n",
            "Test loss: 0.512\n",
            "Test error: 33.000%\n",
            "Minibatch train loss at step 500 : 0.47493583\n",
            "Minibatch train error: 32.000%\n",
            "Test loss: 0.526\n",
            "Test error: 38.857%\n",
            "Minibatch train loss at step 550 : 0.4018858\n",
            "Minibatch train error: 25.000%\n",
            "Test loss: 0.525\n",
            "Test error: 30.571%\n",
            "Minibatch train loss at step 600 : 0.45310256\n",
            "Minibatch train error: 25.000%\n",
            "Test loss: 0.502\n",
            "Test error: 29.571%\n",
            "Minibatch train loss at step 650 : 0.5366437\n",
            "Minibatch train error: 32.000%\n",
            "Test loss: 0.532\n",
            "Test error: 33.143%\n",
            "Minibatch train loss at step 700 : 0.49687594\n",
            "Minibatch train error: 36.000%\n",
            "Test loss: 0.521\n",
            "Test error: 32.857%\n",
            "Minibatch train loss at step 750 : 0.45670217\n",
            "Minibatch train error: 28.000%\n",
            "Test loss: 0.525\n",
            "Test error: 33.143%\n",
            "Minibatch train loss at step 800 : 0.40955037\n",
            "Minibatch train error: 24.000%\n",
            "Test loss: 0.514\n",
            "Test error: 32.429%\n",
            "Minibatch train loss at step 850 : 0.4027574\n",
            "Minibatch train error: 24.000%\n",
            "Test loss: 0.532\n",
            "Test error: 36.714%\n",
            "Minibatch train loss at step 900 : 0.3817315\n",
            "Minibatch train error: 23.000%\n",
            "Test loss: 0.519\n",
            "Test error: 32.143%\n",
            "Minibatch train loss at step 950 : 0.41628984\n",
            "Minibatch train error: 21.000%\n",
            "Test loss: 0.508\n",
            "Test error: 35.000%\n"
          ]
        }
      ],
      "source": [
        "# initialize any variables\n",
        "tf.global_variables_initializer().run(session=session)\n",
        "\n",
        "# load our data and separate it into tweets and labels\n",
        "train_data = json.load(open('data/trainTweets_preprocessed.json', 'r'))\n",
        "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
        "train_tweets = np.array([t[0] for t in train_data])\n",
        "train_labels = np.array([int(t[1]) for t in train_data])\n",
        "\n",
        "test_data = json.load(open('data/testTweets_preprocessed.json', 'r'))\n",
        "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
        "# we are just taking the first 1000 things from the test set for faster evaluation\n",
        "test_data = test_data[0:1000]\n",
        "test_tweets = np.array([t[0] for t in test_data])\n",
        "one_hot_test_tweets = utils.one_hot(test_tweets, vocab_size)\n",
        "test_labels = np.array([int(t[1]) for t in test_data])\n",
        "\n",
        "# we'll train with batches of size 128.  This means that we run\n",
        "# our model on 128 examples and then do gradient descent based on the loss\n",
        "# over those 128 examples.\n",
        "batch_size = 128\n",
        "num_steps = 1000\n",
        "\n",
        "for step in range(num_steps):\n",
        "    # get data for a batch\n",
        "    offset = (step * batch_size) % (len(train_data) - batch_size)\n",
        "    batch_tweets = utils.one_hot(train_tweets[offset : (offset + batch_size)], vocab_size)\n",
        "    batch_labels = train_labels[offset : (offset + batch_size)]\n",
        "\n",
        "    # put this data into a dictionary that we feed in when we run\n",
        "    # the graph.  this data fills in the placeholders we made in the graph.\n",
        "    data = {tweets: batch_tweets, labels: batch_labels}\n",
        "\n",
        "    # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
        "    _, loss_value_train, error_value_train = session.run(\n",
        "      [optimizer, loss, pred_err], feed_dict=data)\n",
        "\n",
        "    # print stuff every 50 steps to see how we are doing\n",
        "    if (step % 50 == 0):\n",
        "        print(\"Minibatch train loss at step\", step, \":\", loss_value_train)\n",
        "        print(\"Minibatch train error: %.3f%%\" % error_value_train)\n",
        "\n",
        "        # get test evaluation\n",
        "        test_loss = []\n",
        "        test_error = []\n",
        "        for batch_num in range(int(len(test_data)/batch_size)):\n",
        "            test_offset = (batch_num * batch_size) % (len(test_data) - batch_size)\n",
        "            test_batch_tweets = one_hot_test_tweets[test_offset : (test_offset + batch_size)]\n",
        "            test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
        "            data_testing = {tweets: test_batch_tweets, labels: test_batch_labels}\n",
        "            loss_value_test, error_value_test = session.run([loss, pred_err], feed_dict=data_testing)\n",
        "            test_loss.append(loss_value_test)\n",
        "            test_error.append(error_value_test)\n",
        "\n",
        "        print(\"Test loss: %.3f\" % np.mean(test_loss))\n",
        "        print(\"Test error: %.3f%%\" % np.mean(test_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zXSfq2jgTpL9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8kSChRB_TpL9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hB-wiydQQlWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OoRq568QlTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzyAymL0QlQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9BF2QqnkQlOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHbTynQSTpL9"
      },
      "source": [
        "## Solutions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C0Tie-oTpL-",
        "outputId": "7b7fc6b9-86f9-4343-d936-7554c8278418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Minibatch train loss at step', 0, ':', 0.6933049)\n",
            "Minibatch train error: 40.000%\n",
            "Test loss: 0.693\n",
            "Test error: 28.400%\n",
            "('Minibatch train loss at step', 50, ':', 0.53027713)\n",
            "Minibatch train error: 15.000%\n",
            "Test loss: 0.629\n",
            "Test error: 22.800%\n",
            "('Minibatch train loss at step', 100, ':', 0.57602787)\n",
            "Minibatch train error: 23.000%\n",
            "Test loss: 0.589\n",
            "Test error: 20.667%\n",
            "('Minibatch train loss at step', 150, ':', 0.5539102)\n",
            "Minibatch train error: 14.000%\n",
            "Test loss: 0.581\n",
            "Test error: 18.733%\n",
            "('Minibatch train loss at step', 200, ':', 0.50935715)\n",
            "Minibatch train error: 16.000%\n",
            "Test loss: 0.548\n",
            "Test error: 17.400%\n",
            "('Minibatch train loss at step', 250, ':', 0.44617847)\n",
            "Minibatch train error: 16.000%\n",
            "Test loss: 0.532\n",
            "Test error: 16.467%\n",
            "('Minibatch train loss at step', 300, ':', 0.587883)\n",
            "Minibatch train error: 17.000%\n",
            "Test loss: 0.533\n",
            "Test error: 16.267%\n",
            "('Minibatch train loss at step', 350, ':', 0.54082739)\n",
            "Minibatch train error: 20.000%\n",
            "Test loss: 0.545\n",
            "Test error: 17.200%\n",
            "('Minibatch train loss at step', 400, ':', 0.56833553)\n",
            "Minibatch train error: 18.000%\n",
            "Test loss: 0.522\n",
            "Test error: 15.733%\n",
            "('Minibatch train loss at step', 450, ':', 0.57482541)\n",
            "Minibatch train error: 19.000%\n",
            "Test loss: 0.533\n",
            "Test error: 16.400%\n",
            "('Minibatch train loss at step', 500, ':', 0.55153328)\n",
            "Minibatch train error: 18.000%\n",
            "Test loss: 0.516\n",
            "Test error: 16.133%\n",
            "('Minibatch train loss at step', 550, ':', 0.57234359)\n",
            "Minibatch train error: 21.000%\n",
            "Test loss: 0.513\n",
            "Test error: 16.133%\n",
            "('Minibatch train loss at step', 600, ':', 0.41946551)\n",
            "Minibatch train error: 11.000%\n",
            "Test loss: 0.520\n",
            "Test error: 16.867%\n",
            "('Minibatch train loss at step', 650, ':', 0.54832065)\n",
            "Minibatch train error: 16.000%\n",
            "Test loss: 0.516\n",
            "Test error: 15.467%\n",
            "('Minibatch train loss at step', 700, ':', 0.46384442)\n",
            "Minibatch train error: 15.000%\n",
            "Test loss: 0.505\n",
            "Test error: 15.267%\n",
            "('Minibatch train loss at step', 750, ':', 0.58556581)\n",
            "Minibatch train error: 16.000%\n",
            "Test loss: 0.506\n",
            "Test error: 15.733%\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import pickle as p\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import utils\n",
        "\n",
        "# set variables\n",
        "tweet_size = 20\n",
        "hidden_size = 100\n",
        "vocab_size = 7597\n",
        "batch_size = 64\n",
        "\n",
        "# this just makes sure that all our following operations will be placed in the right graph.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# create a session variable that we can run later.\n",
        "session = tf.Session()\n",
        "\n",
        "# make placeholders for data we'll feed in\n",
        "tweets = tf.placeholder(tf.float32, [None, tweet_size, vocab_size])\n",
        "labels = tf.placeholder(tf.float32, [None])\n",
        "\n",
        "# make the lstm cells, and wrap them in MultiRNNCell for multiple layers\n",
        "lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
        "multi_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell] * 2, state_is_tuple=True)\n",
        "\n",
        "# define the op that runs the LSTM, across time, on the data\n",
        "_, final_state = tf.nn.dynamic_rnn(multi_lstm_cells, tweets, dtype=tf.float32)\n",
        "\n",
        "# a useful function that takes an input and what size we want the output\n",
        "# to be, and multiples the input by a weight matrix plus bias (also creating\n",
        "# these variables)\n",
        "def linear(input_, output_size, name, init_bias=0.0):\n",
        "    shape = input_.get_shape().as_list()\n",
        "    with tf.variable_scope(name):\n",
        "        W = tf.get_variable(\"weight_matrix\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n",
        "    if init_bias is None:\n",
        "        return tf.matmul(input_, W)\n",
        "    with tf.variable_scope(name):\n",
        "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
        "    return tf.matmul(input_, W) + b\n",
        "\n",
        "# define that our final sentiment logit is a linear function of the final state\n",
        "# of the LSTM\n",
        "sentiment = linear(final_state[-1][-1], 1, name=\"output\")\n",
        "\n",
        "\n",
        "sentiment = tf.squeeze(sentiment, [1])\n",
        "\n",
        "# define cross entropy loss function\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "# round our actual probabilities to compute error\n",
        "prob = tf.nn.sigmoid(sentiment)\n",
        "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
        "pred_err = tf.to_float(tf.not_equal(prediction, labels))\n",
        "pred_err = tf.reduce_sum(pred_err)\n",
        "\n",
        "# define our optimizer to minimize the loss\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "# initialize any variables\n",
        "tf.global_variables_initializer().run(session=session)\n",
        "\n",
        "# load our data and separate it into tweets and labels\n",
        "train_data = json.load(open('data/trainTweets_preprocessed.json', 'r'))\n",
        "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
        "\n",
        "train_tweets = np.array([t[0] for t in train_data])\n",
        "train_labels = np.array([int(t[1]) for t in train_data])\n",
        "\n",
        "test_data = json.load(open('data/testTweets_preprocessed.json', 'r'))\n",
        "test_data = map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data)\n",
        "# we are just taking the first 1000 things from the test set for faster evaluation\n",
        "test_data = test_data[0:1000]\n",
        "test_tweets = np.array([t[0] for t in test_data])\n",
        "one_hot_test_tweets = utils.one_hot(test_tweets, vocab_size)\n",
        "test_labels = np.array([int(t[1]) for t in test_data])\n",
        "\n",
        "# we'll train with batches of size 128.  This means that we run\n",
        "# our model on 128 examples and then do gradient descent based on the loss\n",
        "# over those 128 examples.\n",
        "num_steps = 1000\n",
        "\n",
        "for step in range(num_steps):\n",
        "    # get data for a batch\n",
        "    offset = (step * batch_size) % (len(train_data) - batch_size)\n",
        "    batch_tweets = utils.one_hot(train_tweets[offset : (offset + batch_size)], vocab_size)\n",
        "    batch_labels = train_labels[offset : (offset + batch_size)]\n",
        "\n",
        "    # put this data into a dictionary that we feed in when we run\n",
        "    # the graph.  this data fills in the placeholders we made in the graph.\n",
        "    data = {tweets: batch_tweets, labels: batch_labels}\n",
        "\n",
        "    # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
        "    _, loss_value_train, error_value_train = session.run(\n",
        "      [optimizer, loss, pred_err], feed_dict=data)\n",
        "\n",
        "    # print stuff every 50 steps to see how we are doing\n",
        "    if (step % 50 == 0):\n",
        "        print(\"Minibatch train loss at step\", step, \":\", loss_value_train)\n",
        "        print(\"Minibatch train error: %.3f%%\" % error_value_train)\n",
        "\n",
        "        # get test evaluation\n",
        "        test_loss = []\n",
        "        test_error = []\n",
        "        for batch_num in range(int(len(test_data)/batch_size)):\n",
        "            test_offset = (batch_num * batch_size) % (len(test_data) - batch_size)\n",
        "            test_batch_tweets = one_hot_test_tweets[test_offset : (test_offset + batch_size)]\n",
        "            test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
        "            data_testing = {tweets: test_batch_tweets, labels: test_batch_labels}\n",
        "            loss_value_test, error_value_test = session.run([loss, pred_err], feed_dict=data_testing)\n",
        "            test_loss.append(loss_value_test)\n",
        "            test_error.append(error_value_test)\n",
        "\n",
        "        print(\"Test loss: %.3f\" % np.mean(test_loss))\n",
        "        print(\"Test error: %.3f%%\" % np.mean(test_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8W5JX8W6TpL-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "name": "_merged",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}